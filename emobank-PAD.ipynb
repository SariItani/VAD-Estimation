{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Step 1: Import Libraries"]},{"cell_type":"code","execution_count":122,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-23T06:20:00.625332Z","iopub.status.busy":"2024-09-23T06:20:00.624887Z","iopub.status.idle":"2024-09-23T06:20:02.203681Z","shell.execute_reply":"2024-09-23T06:20:02.202246Z","shell.execute_reply.started":"2024-09-23T06:20:00.625289Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import Ridge\n","from sklearn.metrics import mean_squared_error\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.neural_network import MLPRegressor\n","import requests\n","import io\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"markdown","metadata":{},"source":["# Step 2: Load Dataset"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[],"source":["def get_dataset(url = \"https://raw.githubusercontent.com/JULIELab/EmoBank/master/corpus/emobank.csv\"):\n","    response = requests.get(url)\n","    if response.status_code == 200:\n","        return pd.read_csv(io.StringIO(response.text))\n","    else:\n","        raise Exception(f\"Failed to download the dataset. Status code: {response.status_code}\")"]},{"cell_type":"code","execution_count":124,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T06:20:02.209006Z","iopub.status.busy":"2024-09-23T06:20:02.208489Z","iopub.status.idle":"2024-09-23T06:20:02.295000Z","shell.execute_reply":"2024-09-23T06:20:02.293719Z","shell.execute_reply.started":"2024-09-23T06:20:02.208972Z"},"trusted":true},"outputs":[],"source":["data = get_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Step 3: Exploratory Data Analysis\n","\n","Visualize the distribution of Valence, Arousal, and Dominance"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T06:20:02.297172Z","iopub.status.busy":"2024-09-23T06:20:02.296772Z","iopub.status.idle":"2024-09-23T06:20:04.155468Z","shell.execute_reply":"2024-09-23T06:20:04.154299Z","shell.execute_reply.started":"2024-09-23T06:20:02.297138Z"},"trusted":true},"outputs":[],"source":["sns.histplot(data['V'], kde=True).set_title('Distribution of Valence')\n","plt.show()\n","\n","sns.histplot(data['A'], kde=True).set_title('Distribution of Arousal')\n","plt.show()\n","\n","sns.histplot(data['D'], kde=True).set_title('Distribution of Dominance')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Step 4: Preprocessing\n","\n","Split into training and testing sets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Drop rows with NaN in V or A\n","data = data.dropna(subset=['V', 'A', 'D'])\n","\n","# Scatter plot for Valence (V) and Arousal (A)\n","sns.scatterplot(x='V', y='A', data=data, alpha=0.6)\n","\n","# Customize the plot\n","plt.xlabel(\"Valence (V)\")\n","plt.ylabel(\"Arousal (A)\")\n","plt.title(\"Valence vs. Arousal\")\n","\n","# Add grid lines\n","plt.grid(True)\n","\n","# Show the plot\n","plt.show()\n","\n","# Scatter plot for Valence (V) and Dominance (D)\n","sns.scatterplot(x='V', y='D', data=data, alpha=0.6)\n","\n","# Customize the plot\n","plt.xlabel(\"Valence (V)\")\n","plt.ylabel(\"Arousal (D)\")\n","plt.title(\"Valence vs. Dominance\")\n","\n","# Add grid lines\n","plt.grid(True)\n","\n","# Show the plot\n","plt.show()\n","\n","# Scatter plot for Arousal (A) and Dominance (D)\n","sns.scatterplot(x='V', y='A', data=data, alpha=0.6)\n","\n","# Customize the plot\n","plt.xlabel(\"Valence (V)\")\n","plt.ylabel(\"Arousal (A)\")\n","plt.title(\"Arousal vs. Dominance\")\n","\n","# Add grid lines\n","plt.grid(True)\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a figure for 3D plot\n","fig = plt.figure(figsize=(10, 7))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","ax.scatter(data['V'], data['A'], data['D'], color='blue', label='Actual VAD', alpha=0.3)\n","\n","ax.set_xlabel('Valence (V)')\n","ax.set_ylabel('Arousal (A)')\n","ax.set_zlabel('Dominance (D)')\n","ax.set_title('3D Scatter Plot of VAD Distributions')\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":129,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T06:20:04.497037Z","iopub.status.busy":"2024-09-23T06:20:04.496679Z","iopub.status.idle":"2024-09-23T06:20:04.820421Z","shell.execute_reply":"2024-09-23T06:20:04.818797Z","shell.execute_reply.started":"2024-09-23T06:20:04.497006Z"},"trusted":true},"outputs":[],"source":["# Define a function to remove outliers using IQR\n","def remove_outliers(df, column):\n","    Q1 = df[column].quantile(0.25)\n","    Q3 = df[column].quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower_bound = Q1 - 1.5 * IQR\n","    upper_bound = Q3 + 1.5 * IQR\n","    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n","\n","# Apply the function to Valence (V), Arousal (A), and Dominance (D)\n","data = remove_outliers(data, 'V')\n","data = remove_outliers(data, 'A')\n","data = remove_outliers(data, 'D')\n","\n","# Replace NaN with a placeholder (e.g., an empty string)\n","data['text'] = data['text'].fillna('')\n","\n","# Continue with train-test split and TF-IDF processing\n","train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n","\n","# --- Preprocessing options: CountVectorizer, TF-IDF, Word2Vec ---\n","\n","# Option 1: Using CountVectorizer for LDA (original approach)\n","\n","# count_vectorizer = CountVectorizer(max_features=5000, stop_words='english')\n","# data_counts = count_vectorizer.fit_transform(data['text'])\n","# X_train_counts = count_vectorizer.transform(train_data['text'])\n","# X_test_counts = count_vectorizer.transform(test_data['text'])\n","# # Applying LDA\n","# lda = LatentDirichletAllocation(n_components=20, random_state=42)\n","# lda.fit_transform(data_counts)\n","# X_train = lda.transform(X_train_counts)\n","# X_test = lda.transform(X_test_counts)\n","\n","# Option 2: Using TF-IDF (new approach)\n","\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n","data_tfidf = tfidf_vectorizer.fit_transform(data['text'])\n","X_train = tfidf_vectorizer.transform(train_data['text'])\n","X_test = tfidf_vectorizer.transform(test_data['text'])\n","\n","# Option 3: Using Word2Vec (new approach)\n","\n","# from gensim.models import Word2Vec\n","# def preprocess_word2vec(corpus):\n","#     return [sentence.split() for sentence in corpus]\n","\n","# # Train Word2Vec on the training corpus\n","# word2vec_model = Word2Vec(sentences=preprocess_word2vec(train_data['text']), vector_size=100, window=5, min_count=2, workers=4)\n","\n","# def get_word2vec_embedding(text):\n","#     words = text.split()\n","#     vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n","#     if len(vectors) > 0:\n","#         return np.mean(vectors, axis=0)\n","#     else:\n","#         return np.zeros(word2vec_model.vector_size)\n","\n","# # Transform the training and testing datasets into Word2Vec embeddings\n","# X_train = np.array([get_word2vec_embedding(text) for text in train_data['text']])\n","# X_test = np.array([get_word2vec_embedding(text) for text in test_data['text']])\n","\n","# Choose targets for Valence, Arousal, and Dominance\n","v_train, v_test = train_data['V'], test_data['V']\n","a_train, a_test = train_data['A'], test_data['A']\n","d_train, d_test = train_data['D'], test_data['D']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.histplot(data['V'], kde=True).set_title('Distribution of Valence')\n","plt.show()\n","\n","sns.histplot(data['A'], kde=True).set_title('Distribution of Arousal')\n","plt.show()\n","\n","sns.histplot(data['D'], kde=True).set_title('Distribution of Dominance')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Step 5: Build Models\n","\n","# 5.1 Ridge"]},{"cell_type":"code","execution_count":131,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T06:20:04.822511Z","iopub.status.busy":"2024-09-23T06:20:04.822104Z","iopub.status.idle":"2024-09-23T06:20:04.868108Z","shell.execute_reply":"2024-09-23T06:20:04.866895Z","shell.execute_reply.started":"2024-09-23T06:20:04.822461Z"},"trusted":true},"outputs":[],"source":["model_v = Ridge(alpha=1.0)\n","model_v.fit(X_train, v_train)\n","\n","model_a = Ridge(alpha=1.0)\n","model_a.fit(X_train, a_train)\n","\n","model_d = Ridge(alpha=1.0)\n","model_d.fit(X_train, d_train)\n","\n","ridge_model = (model_v, model_a, model_d)"]},{"cell_type":"markdown","metadata":{},"source":["## 5.2 Random Forest"]},{"cell_type":"code","execution_count":132,"metadata":{},"outputs":[],"source":["model_v = RandomForestRegressor(n_estimators=100, random_state=42)\n","model_v.fit(X_train, v_train)\n","\n","model_a = RandomForestRegressor(n_estimators=100, random_state=42)\n","model_a.fit(X_train, a_train)\n","\n","model_d = RandomForestRegressor(n_estimators=100, random_state=42)\n","model_d.fit(X_train, d_train)\n","\n","rf_model = (model_v, model_a, model_d)"]},{"cell_type":"markdown","metadata":{},"source":["## 5.3 Neural Network Regression"]},{"cell_type":"code","execution_count":133,"metadata":{},"outputs":[],"source":["model_v = MLPRegressor(hidden_layer_sizes=(50, 30), activation='relu', solver='adam', random_state=42, max_iter=500)\n","model_v.fit(X_train, v_train)\n","\n","model_a = MLPRegressor(hidden_layer_sizes=(50, 30), activation='relu', solver='adam', random_state=42, max_iter=500)\n","model_a.fit(X_train, a_train)\n","\n","model_d = MLPRegressor(hidden_layer_sizes=(50, 30), activation='relu', solver='adam', random_state=42, max_iter=500)\n","model_d.fit(X_train, d_train)\n","\n","mlp_model = (model_v, model_a, model_d)"]},{"cell_type":"code","execution_count":134,"metadata":{},"outputs":[],"source":["models = [ridge_model, rf_model, mlp_model]\n","# models = [rf_model]"]},{"cell_type":"markdown","metadata":{},"source":["# Step 6: Evaluate the Models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T06:20:04.962536Z","iopub.status.busy":"2024-09-23T06:20:04.962073Z","iopub.status.idle":"2024-09-23T06:20:05.238862Z","shell.execute_reply":"2024-09-23T06:20:05.237531Z","shell.execute_reply.started":"2024-09-23T06:20:04.962496Z"},"trusted":true},"outputs":[],"source":["from mpl_toolkits.mplot3d import Axes3D\n","\n","for model in models:\n","    print(f\"For model:\", model)\n","    (model_v, model_a, model_d) = model\n","\n","    v_pred = model_v.predict(X_test)\n","\n","    # Calculate Mean Squared Error (MSE)\n","    mse = mean_squared_error(v_test, v_pred)\n","    print(\"Mean Squared Error:\", mse)\n","\n","    # Visualize predictions vs. true values\n","    plt.scatter(v_test, v_pred, alpha=0.6)\n","    plt.xlabel(\"True Valence Values\")\n","    plt.ylabel(\"Valence Predictions\")\n","    plt.title(\"True Valence Values vs. Predictions\")\n","    plt.show()\n","    \n","    a_pred = model_a.predict(X_test)\n","\n","    # Calculate Mean Squared Error (MSE)\n","    mse = mean_squared_error(a_test, a_pred)\n","    print(\"Mean Squared Error:\", mse)\n","\n","    # Visualize predictions vs. true values\n","    plt.scatter(a_test, a_pred, alpha=0.5)\n","    plt.xlabel(\"True Arousal Values\")\n","    plt.ylabel(\"Arousal Predictions\")\n","    plt.title(\"True Arousal Values vs. Predictions\")\n","    plt.show()\n","\n","    d_pred = model_d.predict(X_test)\n","\n","    # Calculate Mean Squared Error (MSE)\n","    mse = mean_squared_error(d_test, d_pred)\n","    print(\"Mean Squared Error:\", mse)\n","\n","    # Visualize predictions vs. true values\n","    plt.scatter(d_test, d_pred, alpha=0.6)\n","    plt.xlabel(\"True Dominance Values\")\n","    plt.ylabel(\"Dominance Predictions\")\n","    plt.title(\"True Dominance Values vs. Predictions\")\n","    plt.show()\n","\n","    # Create a figure for 3D plot\n","    fig = plt.figure(figsize=(10, 7))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Scatter true VAD values (from the test set)\n","    ax.scatter(v_test, a_test, d_test, color='blue', label='Actual VAD', alpha=0.3)\n","\n","    # Scatter predicted VAD values\n","    ax.scatter(v_pred, a_pred, d_pred, color='red', label='Predicted VAD', alpha=0.4)\n","\n","    # Set labels\n","    ax.set_xlabel('Valence (V)')\n","    ax.set_ylabel('Arousal (A)')\n","    ax.set_zlabel('Dominance (D)')\n","    ax.set_title('3D Scatter Plot of Predicted vs. Actual VAD')\n","\n","    # Add a legend\n","    ax.legend()\n","\n","    # Show the plot\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Step7: Making Predictions"]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[],"source":["def predict(message, model):\n","    (model_v, model_a, model_d) = model\n","    v = model_v.predict(message)\n","    a = model_a.predict(message)\n","    d = model_d.predict(message)\n","    return v, a, d"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T06:21:21.311742Z","iopub.status.busy":"2024-09-23T06:21:21.311252Z","iopub.status.idle":"2024-09-23T06:21:21.317114Z","shell.execute_reply":"2024-09-23T06:21:21.315826Z","shell.execute_reply.started":"2024-09-23T06:21:21.311708Z"},"trusted":true},"outputs":[],"source":["# Get a few sample texts from the test data for prediction\n","sampled_data = test_data.sample(25)  # Select 5 samples from the test data\n","# sampled_data = test_data\n","messages = sampled_data['text'].values\n","\n","# --- Handling LDA-specific instances ---\n","\n","# If using LDA:\n","# input_message = lda.transform(count_vectorizer.transform(messages))\n","\n","# If using TF-IDF:\n","input_message = tfidf_vectorizer.transform(messages)\n","\n","# If using Word2Vec:\n","# input_message = np.array([get_word2vec_embedding(text) for text in messages])\n","\n","\n","for model in models:\n","    print(f\"For model:\", model)\n","    (model_v, model_a, model_d) = model\n","\n","    # Get predictions for the new messages\n","    v_pred, a_pred, d_pred = predict(input_message, model)\n","\n","    # Display results for each message\n","    for i, msg in enumerate(messages):\n","        print(f\"Message: {msg}\")\n","        print(f\"Predicted Valence: {v_pred[i]:.2f}, Arousal: {a_pred[i]:.2f}, Dominance: {d_pred[i]:.2f}\")\n","        print(f\"Actual Valence: {sampled_data['V'].values[i]:.2f}, Arousal: {sampled_data['A'].values[i]:.2f}, Dominance: {sampled_data['D'].values[i]:.2f}\\n\")\n","\n","    # Create a 3D plot\n","    fig = plt.figure(figsize=(10, 7))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Actual VAD values for the 5 sampled messages\n","    v_actual = sampled_data['V'].values\n","    a_actual = sampled_data['A'].values\n","    d_actual = sampled_data['D'].values\n","\n","    # Scatter actual values\n","    ax.scatter(v_actual, a_actual, d_actual, color='blue', label='Actual VAD', alpha=0.6, s=100)\n","\n","    # Scatter predicted values\n","    ax.scatter(v_pred, a_pred, d_pred, color='red', label='Predicted VAD', alpha=0.6, s=100)\n","\n","    # Set labels and title\n","    ax.set_xlabel('Valence (V)')\n","    ax.set_ylabel('Arousal (A)')\n","    ax.set_zlabel('Dominance (D)')\n","    ax.set_title('3D Scatter Plot of Predicted vs. Actual VAD for Sampled Messages')\n","\n","    # Add a legend\n","    ax.legend()\n","\n","    # Show the plot\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Step8: Infering Emotions \n","## (Merhabian VAD)"]},{"cell_type":"code","execution_count":138,"metadata":{},"outputs":[],"source":["def normalize_vad(v, a, d):\n","    \"\"\"Normalize VAD values from EmoBank's 1-5 scale to -1 to 1 scale\"\"\"\n","    return (v - 3) / 2, (a - 3) / 2, (d - 3) / 2\n","\n","emotion_coords = {\n","    'joy': (0.76, 0.48, 0.35),\n","    'anger': (-0.51, 0.59, 0.25),\n","    'fear': (-0.64, 0.60, -0.43),\n","    'sadness': (-0.63, -0.27, -0.33),\n","    'surprise': (0.40, 0.67, -0.13),\n","    'disgust': (-0.60, 0.35, 0.11),\n","    'contentment': (0.82, -0.18, 0.21),\n","    'boredom': (-0.65, -0.62, -0.33),\n","    'acceptance': (0.46, -0.09, -0.19)\n","}\n","\n","def infer_emotion_mehrabian(v, a, d):\n","    v_norm, a_norm, d_norm = normalize_vad(v, a, d)\n","    vad = np.array([v_norm, a_norm, d_norm])\n","    \n","    distances = {emotion: np.linalg.norm(vad - np.array(coords)) \n","                 for emotion, coords in emotion_coords.items()}\n","    \n","    return min(distances, key=distances.get)\n","\n","# Function to get the PAD values for a given emotion\n","def get_pad_values(emotion):\n","    if emotion in emotion_coords:\n","        return emotion_coords[emotion]\n","    else:\n","        return None\n","\n","# Function to find the closest emotions given PAD values\n","def find_closest_emotions(v, a, d, n=3):\n","    v_norm, a_norm, d_norm = normalize_vad(v, a, d)\n","    vad = np.array([v_norm, a_norm, d_norm])\n","    \n","    distances = {emotion: np.linalg.norm(vad - np.array(coords)) \n","                 for emotion, coords in emotion_coords.items()}\n","    \n","    sorted_emotions = sorted(distances.items(), key=lambda x: x[1])\n","    return [emotion for emotion, _ in sorted_emotions[:n]]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["v_pred, a_pred, d_pred = predict(input_message, rf_model)\n","\n","# Create a DataFrame to store the results\n","results = pd.DataFrame({\n","    'Message': sampled_data['text'].values,\n","    'Actual_V': sampled_data['V'].values,\n","    'Actual_A': sampled_data['A'].values,\n","    'Actual_D': sampled_data['D'].values,\n","    'Predicted_V': v_pred,\n","    'Predicted_A': a_pred,\n","    'Predicted_D': d_pred\n","})\n","\n","# Infer emotions and find closest emotions for actual and predicted VAD values\n","results['Actual_Emotion'] = results.apply(lambda row: infer_emotion_mehrabian(row['Actual_V'], row['Actual_A'], row['Actual_D']), axis=1)\n","results['Predicted_Emotion'] = results.apply(lambda row: infer_emotion_mehrabian(row['Predicted_V'], row['Predicted_A'], row['Predicted_D']), axis=1)\n","results['Actual_Top3'] = results.apply(lambda row: find_closest_emotions(row['Actual_V'], row['Actual_A'], row['Actual_D']), axis=1)\n","results['Predicted_Top3'] = results.apply(lambda row: find_closest_emotions(row['Predicted_V'], row['Predicted_A'], row['Predicted_D']), axis=1)\n","\n","# Display the results\n","pd.set_option('display.max_colwidth', None)\n","print(results[['Message', 'Actual_Top3', 'Predicted_Top3']])\n","\n","# Calculate accuracy\n","accuracy = (results['Actual_Emotion'] == results['Predicted_Emotion']).mean()\n","print(f\"\\nAccuracy of emotion prediction: {accuracy:.2%}\")\n","\n","# Calculate top-3 accuracy\n","top3_accuracy = results.apply(lambda row: row['Actual_Emotion'] in row['Predicted_Top3'], axis=1).mean()\n","print(f\"Top-3 accuracy of emotion prediction: {top3_accuracy:.2%}\")\n","\n","from sklearn.metrics import confusion_matrix\n","\n","cm = confusion_matrix(results['Actual_Emotion'], results['Predicted_Emotion'], labels=list(emotion_coords.keys()))\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', xticklabels=emotion_coords.keys(), yticklabels=emotion_coords.keys())\n","plt.title('Confusion Matrix of Emotion Prediction')\n","plt.xlabel('Predicted Emotion')\n","plt.ylabel('Actual Emotion')\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4891343,"sourceId":8244807,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
